# AI_Research 技術存檔

### FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts
- **品質**: 【技術增量】 (0.3)
- **日期**: 2026-01-08
- **摘要**: Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graph...
- **連結**: http://arxiv.org/abs/2601.05174v1

---
### DocDancer: Towards Agentic Document-Grounded Information Seeking
- **品質**: 【技術增量】 (0.3)
- **日期**: 2026-01-08
- **摘要**: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as...
- **連結**: http://arxiv.org/abs/2601.05163v1

---
### How to Set the Learning Rate for Large-Scale Pre-training?
- **品質**: 【技術增量】 (0.3)
- **日期**: 2026-01-08
- **摘要**: Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. ...
- **連結**: http://arxiv.org/abs/2601.05049v1

---
### ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG
- **品質**: 【技術增量】 (0.3)
- **日期**: 2026-01-08
- **摘要**: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'...
- **連結**: http://arxiv.org/abs/2601.05038v1

---
### T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs
- **品質**: 【核心突破】 (0.9)
- **日期**: 2026-01-08
- **摘要**: Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage loca...
- **連結**: http://arxiv.org/abs/2601.04945v1

---
### CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters
- **品質**: 【技術增量】 (0.3)
- **日期**: 2026-01-08
- **摘要**: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic av...
- **連結**: http://arxiv.org/abs/2601.04885v1

---
### ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models
- **品質**: 【核心突破】 (0.6)
- **日期**: 2026-01-08
- **摘要**: Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evalu...
- **連結**: http://arxiv.org/abs/2601.04876v1

---
