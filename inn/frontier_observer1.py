from LongTerm_Memory.MemoryManager import MemoryManager
import arxiv
import feedparser
import json
import datetime
import uuid
import re

class MemoryManager:
    def __init__(self):
        self.storage = [] # é€™è£¡æœªä¾†å¯ä»¥å°æ¥ä½ çš„ Pinecone æˆ– SQLite
        print("MemoryManager å·²å•Ÿå‹•ï¼šé•·æœŸè¨˜æ†¶å­˜å„²å°±ç·’ã€‚")

    def save_context(self, data):
        # å¯¦ä½œä½ çš„ã€Œå°‡å°è©±è½‰åŒ–ç‚ºçµæ§‹åŒ–æ•¸æ“šã€é‚è¼¯
        self.storage.append(data)
        
class KnowledgeFilter:
    """å“è³ªæ§åˆ¶æ¨¡çµ„ï¼šè­˜åˆ¥æŠ€è¡“çªç ´ä¸¦éæ¿¾ç‡ŸéŠ·å™±é ­"""
    def __init__(self):
        # è­˜åˆ¥ã€ŒæŠ€è¡“çªç ´ã€çš„å¼·ä¿¡è™Ÿ
        self.breakthrough_signals = {
            "efficiency": [r"linear complexity", r"O\(n\)", r"scaling law", r"memory-efficient"],
            "architecture": [r"novel architecture", r"parameter-efficient", r"state-space model", r"SSM"],
            "performance": [r"outperforms", r"state-of-the-art", r"SOTA", r"zero-shot bottleneck"]
        }
        # è­˜åˆ¥ã€Œç‡ŸéŠ·å™±é ­ã€çš„ç´…æ——ä¿¡è™Ÿ
        self.hype_signals = [
            r"revolutionary", r"next generation of intelligence", 
            r"surpasses human capabilities", r"the end of transformer"
        ]

    def analyze(self, title, summary):
        text = f"{title} {summary}".lower()
        breakthrough_points = 0
        
        for patterns in self.breakthrough_signals.values():
            for pattern in patterns:
                if re.search(pattern, text):
                    breakthrough_points += 1

        hype_points = sum(1 for pattern in self.hype_signals if re.search(pattern, text))
        quality_score = (breakthrough_points * 0.3) - (hype_points * 0.5)
        
        if quality_score >= 0.6:
            label, advice = "ã€çœŸæ­£çš„çªç ´ã€‘", "å¿…é ˆæ›è¼‰ï¼šæ¶‰åŠåº•å±¤æ¶æ§‹æ”¹é€²ã€‚"
        elif quality_score >= 0.2:
            label, advice = "ã€å¢é‡é€²æ­¥ã€‘", "å¯é¸é–±è®€ï¼šç¾æœ‰æŠ€è¡“çš„å„ªåŒ–ã€‚"
        else:
            label, advice = "ã€ç–‘ä¼¼å™±é ­ã€‘", "ç•¥éï¼šç¼ºä¹å…·é«”æŠ€è¡“æ•¸æ“šã€‚"

        return {"label": label, "score": round(quality_score, 2), "advice": advice}

class FrontierObserver:
    def __init__(self):
        # 2026 å‰æ²¿æŠ€è¡“é—œéµå­—
        self.KEYWORDS = {
            "high": ["Sparse Attention", "Mamba", "SSM", "Ring Attention", "MoE", "Linear Attention"],
            "medium": ["Reasoning capability", "Chain of Thought", "Autonomous Agents", "World Models"],
            "low": ["FlashAttention", "vLLM", "Quantization", "LoRA"]
        }
        self.kf = KnowledgeFilter()
        self.output_file = f"knowledge_base_{datetime.datetime.now().strftime('%Y%m%d')}.jsonl"

    def calculate_importance(self, text):
        """åŸºç¤é—œéµå­—è©•åˆ†é‚è¼¯"""
        score = 0.1
        found_tags = []
        for level, weight in [("high", 0.4), ("medium", 0.2), ("low", 0.1)]:
            for kw in self.KEYWORDS[level]:
                if re.search(rf"\b{kw}\b", text, re.IGNORECASE):
                    score += weight
                    found_tags.append(kw)
        return min(score, 1.0), list(set(found_tags))

    def fetch_arxiv(self, max_results=10):
        print("ğŸ” æ­£åœ¨æƒæ ArXiv ä¸¦åŸ·è¡Œå“è³ªéæ¿¾...")
        search = arxiv.Search(
            query="cat:cs.CL OR cat:cs.AI OR cat:cs.LG",
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        results = []
        for result in search.results():
            full_text = f"{result.title} {result.summary}"
            imp_score, tags = self.calculate_importance(full_text)
            
            if imp_score > 0.3:
                # åŸ·è¡Œå“è³ªéæ¿¾å±¤
                q_report = self.kf.analyze(result.title, result.summary)
                results.append({
                    "id": str(uuid.uuid5(uuid.NAMESPACE_URL, result.entry_id)),
                    "timestamp": result.published.isoformat(),
                    "source": "arxiv",
                    "title": result.title,
                    "quality": q_report,
                    "tags": tags,
                    "summary": result.summary[:300] + "..."
                })
        return results

    def fetch_hf_daily(self):
        print("ğŸ” æ­£åœ¨æƒæ Hugging Face ä¸¦è§£ææ¶æ§‹å‰µæ–°...")
        feed_url = "https://papers.takara.ai/api/feed" # 2026 æ¥å£
        feed = feedparser.parse(feed_url)
        results = []
        for entry in feed.entries:
            imp_score, tags = self.calculate_importance(f"{entry.title} {entry.summary}")
            if imp_score > 0.2:
                q_report = self.kf.analyze(entry.title, entry.summary)
                results.append({
                    "id": str(uuid.uuid5(uuid.NAMESPACE_URL, entry.link)),
                    "timestamp": datetime.datetime.now().isoformat(),
                    "source": "huggingface",
                    "title": entry.title,
                    "quality": q_report,
                    "tags": tags,
                    "url": entry.link
                })
        return results

    def run_sync(self):
        """åŸ·è¡ŒåŒæ­¥ä¸¦ä¿å­˜é«˜å“è³ªçŸ¥è­˜"""
        all_data = self.fetch_arxiv() + self.fetch_hf_daily()
        # åƒ…ä¿å­˜ã€Œå¢é‡é€²æ­¥ã€ä»¥ä¸Šçš„å…§å®¹
        filtered_data = [d for d in all_data if d['quality']['score'] > 0.1]
        
        with open(self.output_file, 'a', encoding='utf-8') as f:
            for item in filtered_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        print(f"âœ… ä»»å‹™å®Œæˆï¼å·²éæ¿¾ä¸¦å¯«å…¥ {len(filtered_data)} æ¢é«˜å“è³ªçŸ¥è­˜ã€‚")

#if __name__ == "__main__":
#    FrontierObserver().run_sync()

    # åœ¨ frontier_observer.py çš„ run_sync çµå°¾åŠ å…¥
if __name__ == "__main__":
    observer = FrontierObserver()
    observer.run_sync()
    
    # --- æ–°å¢é•·æœŸè¨˜æ†¶åŒæ­¥ ---
    print("ğŸ§  æ­£åœ¨å°‡é«˜å“è³ªçŸ¥è­˜å¯«å…¥é•·æœŸè¨˜æ†¶åº«...")
    memory = MemoryManager()
    added_count = memory.sync_all_from_jsonl(observer.output_file)
    print(f"âœ¨ åŒæ­¥å®Œæˆï¼å·²æ›´æ–° {added_count} æ¢æŠ€è¡“å­˜æª”è‡³ LongTerm_Memory/ ç›®éŒ„ã€‚")